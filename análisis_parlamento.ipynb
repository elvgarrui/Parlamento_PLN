{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de la transcripción del pleno del parlamento Andaluz.\n",
    "\n",
    "## Extracción de datos\n",
    "\n",
    "Primero, se ha tratado el PDF y se ha convertido a `.txt`.\n",
    "A continuación, se lee y se almacena en una lista, línea por línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sumtext.txt\",\"r\", encoding=\"utf8\") as file:\n",
    "    lines = file.read()    \n",
    "    \n",
    "lines = lines.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "line_erasers = [\"LENO\",\"\\x0c\",\"úm. 14\",\"XI LEGISLATURA\",\"DSPA_11_014\",\"Pág.\",\"2 de mayo de 2019\",\"11-19/CAU-000001\",\"11-19\"]\n",
    "take_out_words = [\"\\n\",\"—\",\"PLENO\"]\n",
    "\n",
    "for l in lines:\n",
    "    insert = True\n",
    "    for word in take_out_words:\n",
    "        l = l.replace(word,\"\")\n",
    "    for eraser in line_erasers:\n",
    "        if eraser in l:\n",
    "            insert = False\n",
    "    if insert:\n",
    "        document.append(l)\n",
    "\n",
    "# 1 Núm. 14 XI LEGISLATURA 2 de mayo de 201 11-19/DVOT-000012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto intervención servirá para guardar los fragmentos de conversación, con la persona que habla, la linea de inicio, el texto, y la linea final. Guardamos las lineas para poder encontrar luego las respuestas del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intervencion:\n",
    "    def __init__(self,persona,inicio,texto,fin):\n",
    "        self.persona = persona\n",
    "        self.inicio = inicio\n",
    "        self.texto = texto\n",
    "        self.fin = fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un diccionario intervenciones con una estructura `[persona,invervenciones]`, donde persona es el nombre de quien interviene e intervenciones una lista que contiene todas las intervenciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervenciones = {}\n",
    "key = \"\"\n",
    "i = 0\n",
    "texto = \"\"\n",
    "no_dialogo = True\n",
    "persona = \"\"\n",
    "inicio = None\n",
    "fin = None\n",
    "for n_pag,line in enumerate(document):\n",
    "    if (\"El señor\" in line or \"La señora\" in line) and sum(map(str.isupper,line)) > 9 and sum(map(str.islower,line)) < 10:\n",
    "        dialogo = True\n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            persona = line.strip(\"La señora \").strip(\"El señor \")\n",
    "            persona = persona.split(\",\")\n",
    "            if type(persona) == list:\n",
    "                persona = persona[0]\n",
    "            inicio = n_pag\n",
    "        else:\n",
    "            fin = n_pag-1\n",
    "            intervencion = Intervencion(persona,inicio,texto,fin)\n",
    "            persona = line.strip(\"La señora \").strip(\"El señor \")\n",
    "            persona = persona.split(\",\")\n",
    "            if type(persona) == list:\n",
    "                persona = persona[0]\n",
    "            if \"ÓPEZ\" == persona[0:4]:\n",
    "                persona = \"L\"+persona\n",
    "            if \"SPINOSA\" == persona[0:7]:\n",
    "                persona = \"E\"+persona\n",
    "            if \"RAMIREZ DE ARELLANO\" in persona:\n",
    "                persona = \"RAMÍREZ DE ARELLANO\"\n",
    "            if \"MORA GRAND\" == persona:\n",
    "                persona = \"MORA GRANDE\"\n",
    "            if \"FERRIZ GOMEZ\" == persona:\n",
    "                persona = \"FÉRRIZ GÓMEZ\"\n",
    "            if \"SEGOVIA BROM\" == persona:\n",
    "                persona = \"SEGOVIA BROME\"\n",
    "            if \"OPIS\" == persona[0:4]:\n",
    "                persona = \"LL\" + persona\n",
    "            \n",
    "            inicio = n_pag\n",
    "            texto = \"\"\n",
    "            if not intervencion.persona in intervenciones.keys():\n",
    "                intervenciones[intervencion.persona] = [intervencion]\n",
    "            else:\n",
    "                intervenciones[intervencion.persona].append(intervencion)\n",
    "    else:\n",
    "        if i > 0:\n",
    "            if \"11-19/DL-000001\" in line or \"DVOT-000012\" in line:\n",
    "                dialogo = False\n",
    "            if dialogo:\n",
    "                texto+=line + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se puede ver como se guardan los datos de cada objeto intervención, con un ejemplo de la presidenta de la camara, Bosquet Aznar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = intervenciones[\"BOSQUET AZNAR\"][1]\n",
    "print(test.persona,\"\\n\",test.inicio,\"\\n\",test.fin,\"\\n\",test.texto,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos el cuerpo del PDF procesado y organizado, puede comenzarse el análisis. Gracias a el uso de objetos, podrá elegirse qué tipo de intervenciones se quieren analizar en cada momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis superficial del pleno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import pandas as pd\n",
    "py.init_notebook_mode(connected=False)\n",
    "\n",
    "count_interv = {}\n",
    "for interv, n in intervenciones.items():\n",
    "    count_interv[interv] = len(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_count_df = pd.DataFrame.from_dict(count_interv,orient='index')\n",
    "int_count_df.columns = ['count']\n",
    "int_count_df['persona'] = int_count_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación creamos una lista con el conteo de intervenciones de cada diputado del parlamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"partidos.csv\",delimiter=\";\")\n",
    "df = pd.merge(df,int_count_df,on='persona')\n",
    "df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resulta evidente, la presidenta de la cámara tiene, con diferencia, la mayor cantidad de intervenciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfica 1. Intervenciones/persona (Agrupado en partidos)\n",
    "\n",
    "**1.1 Con la presidenta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'PSOE': 'red',\n",
    "          'CIUDADANOS': 'orange',\n",
    "          'VOX': 'lightgreen',\n",
    "          'ADELANTE': 'green',\n",
    "           'PP': 'blue',\n",
    "           'AXSI': 'gray'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://plot.ly/python/bar-charts/\n",
    "import plotly.graph_objs as go\n",
    "bars = []\n",
    "df['apellido1'] = df.persona.str.split().str.get(0)\n",
    "df['apellido1'] += \" \" + df.persona.str.split().str.get(1).str.slice(stop=1)+\".\"\n",
    "\n",
    "\n",
    "for partido,partido_df in df.groupby('partido'):\n",
    "    bars.append(go.Bar(\n",
    "    x = partido_df['apellido1'],\n",
    "    y = partido_df['count'],\n",
    "    text = partido_df['persona'],\n",
    "    name = partido,\n",
    "    marker = {'color': colors[partido]}\n",
    "    ))\n",
    "\n",
    "go.FigureWidget(data = bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperar, ciudadanos tiene el mayor numero de intervenciones, pero esto se debe a que la presidenta de la camara es de este partido. Para nivelar los resultados, se le elimina del conjunto.\n",
    "\n",
    "**1.2 Sin la presidenta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bars_n = []\n",
    "\n",
    "for partido,partido_df in df[df.persona != 'BOSQUET AZNAR'].groupby('partido'):\n",
    "    bars_n.append(go.Bar(\n",
    "    x = partido_df['apellido1'],\n",
    "    y = partido_df['count'],\n",
    "    text = partido_df['persona'],\n",
    "    name = partido,\n",
    "    marker = {'color': colors[partido]}\n",
    "    ))\n",
    "    \n",
    "layout = go.Layout(\n",
    "    title=go.layout.Title(\n",
    "        xref='paper',\n",
    "        x=0\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        title=go.layout.yaxis.Title(\n",
    "            text='Nº Intervenciones',\n",
    "            font=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=18,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig = go.FigureWidget(data = bars_n,layout=layout)\n",
    "py.iplot(fig, filename='grouped-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analizar el lenguaje de cada uno de los participantes, unimos todos sus discursos en un unico corpus. Esto se utilizará primero para hacer una comparación entre la gráfica de intervenciones y la de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "\n",
    "#remover = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "#def remover_punctuacion(text):\n",
    "#      return text.translate(remover)\n",
    "    \n",
    "def limpiar_texto(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('_', ' ', text)\n",
    "    text = ' '.join(re.findall('[a-zA-ZáéíñóúüÁÉÍÑÓÚÜ]+', text))\n",
    "    #text = re.sub('[0-9]+', ' ', text)\n",
    "\n",
    "    #Elimina stopwords    \n",
    "    filtered_words = [word for word in text.split() if (word not in stopwords.words('spanish')) and (word not in STOP_WORDS)]\n",
    "    text = ' '.join(filtered_words)\n",
    "    \n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "textos={}\n",
    "texto_unsplitted = {}\n",
    "\n",
    "#Guarda una lista de listas. Cada conjero tendrá varias listas tokenizadas, cada una correspondiente a una intervención suya.\n",
    "textos_intervenciones = {}\n",
    "\n",
    "for p,inters in intervenciones.items():\n",
    "    textos_intervenciones[p] = []\n",
    "    textos[p] = \"\"\n",
    "    for inter in inters:\n",
    "        textos[p]+= inter.texto\n",
    "        textos_intervenciones[p].append((limpiar_texto(inter.texto)).split())\n",
    "    \n",
    "    textos[p] = limpiar_texto(textos[p])\n",
    "   # textos[p] = textos[p].lower()\n",
    "    texto_unsplitted[p] = textos[p]\n",
    "    textos[p] = textos[p].split()\n",
    "    \n",
    "    \n",
    "    \n",
    "#for inter in inters:\n",
    "#    palabras = limpiar_texto(inter.texto).split()\n",
    "#    if len(palabras)>20:\n",
    "#        textos[p]+=' '.join(palabras)\n",
    "#        textos_intervenciones[p].append(palabras)\n",
    "\n",
    "#texto_unsplitted[p] = ''.join(textos[p])\n",
    "#textos[p] = textos[p].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_palabras = {}\n",
    "for persona,texto in textos.items():\n",
    "    n_palabras[persona] = sum([y for x,y in Counter(texto).items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_palabras = pd.DataFrame.from_dict(n_palabras,orient='index')\n",
    "df_palabras.columns = ['palabras']\n",
    "df_palabras['persona'] = df_palabras.index\n",
    "df_palabras = pd.merge(df,df_palabras,on='persona')\n",
    "df_palabras[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_2 = []\n",
    "\n",
    "for partido,partido_df in df_palabras.groupby('partido'):\n",
    "    bars_2.append(go.Bar(\n",
    "    x = partido_df['apellido1'],\n",
    "    y = partido_df['palabras'],\n",
    "    text = partido_df['persona'],\n",
    "    name = partido,\n",
    "    marker = {'color': colors[partido]}\n",
    "    ))\n",
    "    \n",
    "layout = go.Layout(\n",
    "    title=go.layout.Title(\n",
    "        xref='paper',\n",
    "        x=0\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        title=go.layout.yaxis.Title(\n",
    "            text='Nº palabras',\n",
    "            font=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=18,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "go.FigureWidget(data = bars_2,layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANÁLISIS LINGUISTICO DE LAS INTERVENCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "aplausos={}\n",
    "for p,inters in intervenciones.items():\n",
    "    aplausos[p] = 0\n",
    "    for inter in inters:\n",
    "        corpus = inter.texto\n",
    "        for char in '-.,\\n\\[\\]¿?':\n",
    "            corpus = corpus.replace(char,' ')\n",
    "        corpus = corpus.lower()\n",
    "        corpus = corpus.split()\n",
    "        for word in corpus:\n",
    "            if word==\"aplausos\":\n",
    "                aplausos[p]+=1\n",
    "df_aplausos = pd.DataFrame.from_dict(aplausos,orient='index')\n",
    "df_aplausos.columns = ['aplausos']\n",
    "df_aplausos['persona'] = df_aplausos.index\n",
    "df_aplausos = pd.merge(df,df_aplausos,on='persona')\n",
    "df_aplausos = df_aplausos[df_aplausos.persona != 'BOSQUET AZNAR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "interrup={}\n",
    "for p,inters in intervenciones.items():\n",
    "    interrup[p] = 0\n",
    "    for inter in inters:\n",
    "        corpus = inter.texto\n",
    "        for char in '-.,\\n\\[\\]¿?':\n",
    "            corpus = corpus.replace(char,' ')\n",
    "        corpus = corpus.lower()\n",
    "        corpus = corpus.split()\n",
    "        for word in corpus:\n",
    "            if word==\"rumores\":\n",
    "                interrup[p]+=1\n",
    "df_inter = pd.DataFrame.from_dict(interrup,orient='index')\n",
    "df_inter.columns = ['interrupciones']\n",
    "df_inter['persona'] = df_inter.index\n",
    "df_inter = pd.merge(df_aplausos,df_inter,on='persona')\n",
    "df_inter = df_inter[df_inter.persona != 'BOSQUET AZNAR']\n",
    "\n",
    "df_sum = df_inter.groupby(by=\"partido\").sum()\n",
    "df_sum['partido'] = df_sum.index\n",
    "\n",
    "interrupciones = go.Bar(\n",
    "    y = df_sum['interrupciones'],\n",
    "    x = df_sum['partido'],\n",
    "    name = 'Interrupciones'\n",
    ")\n",
    "aplausos = go.Bar(\n",
    "    y = df_sum['aplausos'],\n",
    "    x = df_sum['partido'],\n",
    "    name = 'Aplausos'\n",
    ")\n",
    "\n",
    "data = [interrupciones, aplausos]\n",
    "\n",
    "layout = go.Layout(\n",
    "    \n",
    "    barmode='group',\n",
    "    yaxis = go.layout.YAxis(\n",
    "        title = go.layout.yaxis.Title(\n",
    "            text='nº veces',\n",
    "            font=dict(\n",
    "                size=15,\n",
    "            ))))\n",
    "\n",
    "a = go.FigureWidget(data=data,layout=layout)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## NO CONSIGO PONER ESTO CON UN SIZE DECENTE\n",
    "from IPython.display import IFrame\n",
    "\n",
    "from plotly import tools\n",
    "\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=len(bars_n),\n",
    "                          shared_xaxes=True, shared_yaxes=True,\n",
    "                          vertical_spacing=0.001)\n",
    "for i,b in enumerate(bars_n):\n",
    "    fig.append_trace(b, 1, i+1)\n",
    "\n",
    "    \n",
    "fig2 = tools.make_subplots(rows=1, cols=len(bars),\n",
    "                          shared_xaxes=True, shared_yaxes=True,\n",
    "                          vertical_spacing=0.001)\n",
    "for i,b in enumerate(bars_2):\n",
    "    fig2.append_trace(b, 1, i+1)\n",
    "\n",
    "fig['layout'].update(height=400, width=1000, title='Intervenciones')\n",
    "fig2['layout'].update(height=400, width=1000, title='Wordcount')\n",
    "\n",
    "py.iplot(go.FigureWidget(fig))\n",
    "py.iplot(go.FigureWidget(fig2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelamiento de tematica (topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#pip install --upgrade gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Texto de ejemplo\n",
    "\n",
    "inter_un = texto_unsplitted[\"CANO PALOMINO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmanización\n",
    "\n",
    "inter_un = [l.lemma_ for l in nlp(inter_un)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Palabras más repetidas\n",
    "\n",
    "def showWordCloud(data):\n",
    "    words = ' '.join(data)\n",
    "    cleaned_word = \" \".join([word for word in words.split()])\n",
    "    wordcloud = WordCloud(stopwords = STOPWORDS,\n",
    "                         background_color = 'black',\n",
    "                         width = 2500,\n",
    "                         height = 2500\n",
    "                         ).generate(cleaned_word)\n",
    "    plt.figure(1,figsize = (13,13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "showWordCloud(inter_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo para el modelado de topicos\n",
    "\n",
    "inter = textos_intervenciones[\"CANO PALOMINO\"]\n",
    "\n",
    "#inter = [l.lemma_ for l in nlp([i for i in inter])]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar las intervenciones con menos de 20 palabras, o 20 palabras\n",
    "\n",
    "inter = [i for i in inter if len(i)>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar frases comunes en las entrada, y las convierte en una sola, es decir palabras que suelan estar juntas:\n",
    "# Ej: señora magistrado = señora_magistrado\n",
    "\n",
    "#tokens = data['tokens'].tolist()\n",
    "bigram_model = Phrases(inter)   \n",
    "trigram_model = Phrases(bigram_model[inter], min_count=1) \n",
    "tokens = list(trigram_model[bigram_model[inter]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary(tokens)\n",
    "#dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "num_topics = 5\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    \n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Obtener el tópico principal de cada documento\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Obtenga el topico dominante, la contribución porcentual y las palabras clave de cada documento\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => topico dominante\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    \n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=tokens)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la pantalla para mostrar más caracteres en la columna\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset indices  \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Mostrar\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Nube de las n palabras top en cada topico\n",
    "from matplotlib import pyplot as plt\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=STOPWORDS,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in tokens for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Conteo de palabras')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Importancia')\n",
    "    ax.set_ylabel('Conteo de palabras', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Número de palabras e importancia de las palabras clave por tema', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyldavis\n",
    "\n",
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary_LDA)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras más comunes por pares de partido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "texto_partido = {'CIUDADANOS':\"\",'VOX':\"\",'PP':\"\",'PSOE':\"\",'ADELANTE':\"\",'AXSI':\"\"}\n",
    "for persona,texto in texto_unsplitted.items():\n",
    "    if persona != 'BOSQUET AZNAR':\n",
    "        for part in texto_partido.keys():\n",
    "            p = df[df.persona == persona].partido.values[0]\n",
    "            if p == part:\n",
    "                texto_partido[part] +=texto\n",
    "\n",
    "split_texto_partido = {'CIUDADANOS':[],'VOX':[],'PP':[],'PSOE':[],'ADELANTE':[],'AXSI':[]}\n",
    "for persona,texto in textos.items():\n",
    "    for part in split_texto_partido.keys():\n",
    "        p = df[df.persona == persona].partido.values[0]\n",
    "        if p == part:\n",
    "            split_texto_partido[part] +=texto\n",
    "\n",
    "trigrams_dict = {'CIUDADANOS':[],'VOX':[],'PP':[],'PSOE':[],'ADELANTE':[],'AXSI':[]}\n",
    "for partido,texto in texto_partido.items():\n",
    "    trigrams =  nltk.trigrams(texto)\n",
    "    #for bg in bigrams:\n",
    "    #    if \"y\" in bg[0]:\n",
    "    #        print(bg)\n",
    "    trigrams_dict[partido] = Counter(trigrams).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download es_core_news_md\n",
    "\n",
    "import scattertext as st\n",
    "\n",
    "doc = spacy_nlp(article)\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "print('Original Article: %s' % (article))\n",
    "print()\n",
    "print(tokens)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "\n",
    "corpus_df = pd.DataFrame(columns = ['partido','texto','parsed','parsed2'])\n",
    "corpus_df['partido'] = texto_partido.keys()\n",
    "corpus_df['texto'] = [y for x,y in texto_partido.items()]\n",
    "corpus_df['parsed'] = [y for x,y in split_texto_partido.items()]\n",
    "corpus_df['parsed2'] = corpus_df.texto.apply(nlp)\n",
    "corpus = st.CorpusFromParsedDocuments(corpus_df, category_col='partido', parsed_col='parsed2').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.texto[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scattertext import CorpusFromPandas, produce_scattertext_explorer\n",
    "\n",
    "html = produce_scattertext_explorer(corpus,\n",
    "                                    category='VOX',\n",
    "                                    category_name='Vox',\n",
    "                                    not_category_name='Otros',\n",
    "                                    width_in_pixels=1000,\n",
    "                                    minimum_term_frequency=10,\n",
    "                                    transform=st.Scalers.scale,\n",
    "                                    #transform=st.Scalers.percentile\n",
    "                                    metadata=corpus_df['partido'])\n",
    "file_name = 'output/plenoAndaluz.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = corpus_df[corpus_df.partido == 'VOX' ]\n",
    "comparison = comparison.append(corpus_df[corpus_df.partido == 'ADELANTE'])\n",
    "corpus = st.CorpusFromParsedDocuments(comparison, category_col='partido', parsed_col='parsed2').build()\n",
    "html = produce_scattertext_explorer(corpus,\n",
    "                                    category='VOX',\n",
    "                                    category_name='Vox',\n",
    "                                    not_category_name='Adelante',\n",
    "                                    width_in_pixels=1000,\n",
    "                                    minimum_term_frequency=5,\n",
    "                                    transform=st.Scalers.percentile)\n",
    "file_name = 'output/vox-adelante.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = corpus_df[corpus_df.partido == 'PSOE' ]\n",
    "comparison = comparison.append(corpus_df[corpus_df.partido == 'ADELANTE'])\n",
    "corpus = st.CorpusFromParsedDocuments(comparison, category_col='partido', parsed_col='parsed2').build()\n",
    "html = produce_scattertext_explorer(corpus,\n",
    "                                    category='PSOE',\n",
    "                                    category_name='Psoe',\n",
    "                                    not_category_name='Adelante',\n",
    "                                    width_in_pixels=1000,\n",
    "                                    minimum_term_frequency=5,\n",
    "                                    transform=st.Scalers.percentile)\n",
    "file_name = 'output/vox-adelante.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = corpus_df[corpus_df.partido == 'PSOE' ]\n",
    "comparison = comparison.append(corpus_df[corpus_df.partido == 'CIUDADANOS'])\n",
    "corpus = st.CorpusFromParsedDocuments(comparison, category_col='partido', parsed_col='parsed2').build()\n",
    "html = produce_scattertext_explorer(corpus,\n",
    "                                    category='PSOE',\n",
    "                                    category_name='Psoe',\n",
    "                                    not_category_name='Cs',\n",
    "                                    width_in_pixels=1000,\n",
    "                                    minimum_term_frequency=5,\n",
    "                                    transform=st.Scalers.percentile)\n",
    "file_name = 'output/vox-adelante.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = corpus_df[corpus_df.partido == 'CIUDADANOS' ]\n",
    "comparison = comparison.append(corpus_df[corpus_df.partido == 'ADELANTE'])\n",
    "corpus = st.CorpusFromParsedDocuments(comparison, category_col='partido', parsed_col='parsed2').build()\n",
    "html = produce_scattertext_explorer(corpus,\n",
    "                                    category='CIUDADANOS',\n",
    "                                    category_name='Cs',\n",
    "                                    not_category_name='Adelante',\n",
    "                                    width_in_pixels=1000,\n",
    "                                    minimum_term_frequency=5,\n",
    "                                    transform=st.Scalers.percentile)\n",
    "file_name = 'output/vox-adelante.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = corpus_df[corpus_df.partido == 'CIUDADANOS' ]\n",
    "comparison.append(corpus_df[corpus_df.partido == 'PP'])\n",
    "corpus = st.CorpusFromParsedDocuments(comparison, category_col='partido', parsed_col='parsed2').build()\n",
    "html = produce_scattertext_explorer(corpus,\n",
    "                                    category='CIUDADANOS',\n",
    "                                    category_name='Cs',\n",
    "                                    not_category_name='PP',\n",
    "                                    width_in_pixels=1000,\n",
    "                                    minimum_term_frequency=5,\n",
    "                                    max_terms=1000,\n",
    "                                    transform=st.Scalers.percentile,\n",
    "                                    metadata=corpus_df['partido'])\n",
    "file_name = 'output/cs-pp.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scattertext import word_similarity_explorer\n",
    "html = word_similarity_explorer(corpus,\n",
    "                                category='Cs',\n",
    "                                category_name='PP',\n",
    "                                not_category_name='Republican',\n",
    "                                target_term='jobs',\n",
    "                                minimum_term_frequency=5,\n",
    "                                pmi_threshold_coefficient=4,\n",
    "                                width_in_pixels=1000,\n",
    "                                metadata=convention_df['speaker'],\n",
    "                                alpha=0.01,\n",
    "                                max_p_val=0.05,\n",
    "                                save_svg_button=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
